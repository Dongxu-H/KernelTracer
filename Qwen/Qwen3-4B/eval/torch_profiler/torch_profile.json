{
    "aten::embedding": [
        {
            "count": 1,
            "cpu_time_total": 92.73599999999988,
            "cuda_time_total": 5.695999999999913,
            "self_cpu_time_total": 26.019000000000233,
            "input_shapes": "[[151936, 2560], [1, 2], [], [], []]",
            "stack": [
                "<built-in method embedding of type object at 0x7efccdac94a0>",
                "torch/nn/functional.py(2432): embedding",
                "torch/nn/modules/sparse.py(191): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Embedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::reshape": [
        {
            "count": 1,
            "cpu_time_total": 6.031999999999243,
            "cuda_time_total": 0,
            "self_cpu_time_total": 4.308999999999287,
            "input_shapes": "[[1, 2], []]",
            "stack": [
                "<built-in method embedding of type object at 0x7efccdac94a0>",
                "torch/nn/functional.py(2432): embedding",
                "torch/nn/modules/sparse.py(191): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Embedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 4.418999999999869,
            "cuda_time_total": 0,
            "self_cpu_time_total": 2.524999999999636,
            "input_shapes": "[[1, 64, 1], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 2.675999999999476,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.66399999999976,
            "input_shapes": "[[1, 1, 2], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 181,
            "cpu_time_total": 785.5020000000186,
            "cuda_time_total": 0,
            "self_cpu_time_total": 470.34400000006644,
            "input_shapes": "[[1, 2, 2560], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_0",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 173.95799999998417,
            "cuda_time_total": 0,
            "self_cpu_time_total": 86.65000000004329,
            "input_shapes": "[[1, 2, 32, 128], []]",
            "stack": [
                "<built-in method reshape of Tensor object at 0x7efa43e4a940>",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 104.16800000002877,
            "cuda_time_total": 0,
            "self_cpu_time_total": 62.42900000004738,
            "input_shapes": "[[1, 2, 4096], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_3",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 132.2260000000697,
            "cuda_time_total": 0,
            "self_cpu_time_total": 79.32300000007672,
            "input_shapes": "[[1, 2, 9728], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_6",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::view": [
        {
            "count": 1,
            "cpu_time_total": 1.7229999999999563,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.7229999999999563,
            "input_shapes": "[[1, 2], []]",
            "stack": [
                "<built-in method embedding of type object at 0x7efccdac94a0>",
                "torch/nn/functional.py(2432): embedding",
                "torch/nn/modules/sparse.py(191): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Embedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 1.0020000000004075,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.0020000000004075,
            "input_shapes": "[[2, 2560], []]",
            "stack": [
                "<built-in method embedding of type object at 0x7efccdac94a0>",
                "torch/nn/functional.py(2432): embedding",
                "torch/nn/modules/sparse.py(191): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Embedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 1.8940000000002328,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.8940000000002328,
            "input_shapes": "[[1, 64, 1], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 1.0119999999997162,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.0119999999997162,
            "input_shapes": "[[1, 1, 2], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 181,
            "cpu_time_total": 315.15799999995215,
            "cuda_time_total": 0,
            "self_cpu_time_total": 315.15799999995215,
            "input_shapes": "[[1, 2, 2560], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_0",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 100.05999999999585,
            "cuda_time_total": 0,
            "self_cpu_time_total": 100.05999999999585,
            "input_shapes": "[[1, 2, 4096], []]",
            "stack": [
                "<built-in method view of Tensor object at 0x7efa43e4b020>",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 120.90899999999237,
            "cuda_time_total": 0,
            "self_cpu_time_total": 120.90899999999237,
            "input_shapes": "[[1, 2, 1024], []]",
            "stack": [
                "<built-in method view of Tensor object at 0x7efa43e4b020>",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 87.30799999994088,
            "cuda_time_total": 0,
            "self_cpu_time_total": 87.30799999994088,
            "input_shapes": "[[1, 2, 32, 128], []]",
            "stack": [
                "<built-in method reshape of Tensor object at 0x7efa43e4a940>",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 52.90299999999297,
            "cuda_time_total": 0,
            "self_cpu_time_total": 52.90299999999297,
            "input_shapes": "[[1, 2, 9728], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_6",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::index_select": [
        {
            "count": 1,
            "cpu_time_total": 59.68299999999999,
            "cuda_time_total": 5.695999999999913,
            "self_cpu_time_total": 27.48199999999906,
            "input_shapes": "[[151936, 2560], [], [2]]",
            "stack": [
                "<built-in method embedding of type object at 0x7efccdac94a0>",
                "torch/nn/functional.py(2432): embedding",
                "torch/nn/modules/sparse.py(191): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Embedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::empty": [
        {
            "count": 218,
            "cpu_time_total": 668.9810000000853,
            "cuda_time_total": 0,
            "self_cpu_time_total": 668.9810000000853,
            "input_shapes": "[[], [], [], [], [], []]",
            "stack": [
                "<built-in method embedding of type object at 0x7efccdac94a0>",
                "torch/nn/functional.py(2432): embedding",
                "torch/nn/modules/sparse.py(191): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Embedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::resize_": [
        {
            "count": 2,
            "cpu_time_total": 14.947000000000116,
            "cuda_time_total": 0,
            "self_cpu_time_total": 14.947000000000116,
            "input_shapes": "[[0], [], []]",
            "stack": [
                "<built-in method embedding of type object at 0x7efccdac94a0>",
                "torch/nn/functional.py(2432): embedding",
                "torch/nn/modules/sparse.py(191): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Embedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "cudaLaunchKernel": [
        {
            "count": 2072,
            "cpu_time_total": 13927.823000000204,
            "cuda_time_total": 0,
            "self_cpu_time_total": 13927.823000000204,
            "input_shapes": "[]",
            "stack": [
                "<built-in method embedding of type object at 0x7efccdac94a0>",
                "torch/nn/functional.py(2432): embedding",
                "torch/nn/modules/sparse.py(191): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Embedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::Half, long, unsigned int, 2, 2, -2>(at::cuda::detail::TensorInfo<c10::Half, unsigned int>, at::cuda::detail::TensorInfo<c10::Half const, unsigned int>, at::cuda::detail::TensorInfo<long const, unsigned int>, int, int, unsigned int, long)": [
        {
            "count": 1,
            "cpu_time_total": 0,
            "cuda_time_total": 5.695999999999913,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in method embedding of type object at 0x7efccdac94a0>",
                "torch/nn/functional.py(2432): embedding",
                "torch/nn/modules/sparse.py(191): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Embedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::arange": [
        {
            "count": 1,
            "cpu_time_total": 36.960000000000036,
            "cuda_time_total": 2.2719999999999345,
            "self_cpu_time_total": 7.063000000000102,
            "input_shapes": "[[], [], [], [], [], []]",
            "stack": [
                "<built-in method arange of type object at 0x7efccdac94a0>",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 24.79699999999957,
            "cuda_time_total": 2.2719999999999345,
            "self_cpu_time_total": 9.819000000000415,
            "input_shapes": "[[], [], [], [0]]",
            "stack": [
                "<built-in method arange of type object at 0x7efccdac94a0>",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, function_traits<at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>::result_type*)": [
        {
            "count": 1,
            "cpu_time_total": 0,
            "cuda_time_total": 2.2719999999999345,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in method arange of type object at 0x7efccdac94a0>",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::unsqueeze": [
        {
            "count": 1,
            "cpu_time_total": 10.791000000000167,
            "cuda_time_total": 0,
            "self_cpu_time_total": 9.47900000000027,
            "input_shapes": "[[2], []]",
            "stack": [
                "<built-in method unsqueeze of Tensor object at 0x7efa43e4da40>",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 5.130000000000109,
            "cuda_time_total": 0,
            "self_cpu_time_total": 4.168000000000575,
            "input_shapes": "[[64], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 3.3870000000006257,
            "cuda_time_total": 0,
            "self_cpu_time_total": 2.756000000000313,
            "input_shapes": "[[1, 64], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 3.095999999999549,
            "cuda_time_total": 0,
            "self_cpu_time_total": 2.7049999999999272,
            "input_shapes": "[[1, 2], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 242.27100000000246,
            "cuda_time_total": 0,
            "self_cpu_time_total": 199.99899999999616,
            "input_shapes": "[[1, 2, 128], []]",
            "stack": [
                "<built-in method unsqueeze of Tensor object at 0x7efa43e4da40>",
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::as_strided": [
        {
            "count": 1,
            "cpu_time_total": 1.3119999999998981,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.3119999999998981,
            "input_shapes": "[[2], [], [], []]",
            "stack": [
                "<built-in method unsqueeze of Tensor object at 0x7efa43e4da40>",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 0.9619999999995343,
            "cuda_time_total": 0,
            "self_cpu_time_total": 0.9619999999995343,
            "input_shapes": "[[64], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 2,
            "cpu_time_total": 1.2620000000006257,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.2620000000006257,
            "input_shapes": "[[1, 64], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 2,
            "cpu_time_total": 1.6829999999999927,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.6829999999999927,
            "input_shapes": "[[1, 64, 1], [], [], []]",
            "stack": [
                "<built-in method expand of Tensor object at 0x7efa43e4dc20>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 2,
            "cpu_time_total": 0.9319999999988795,
            "cuda_time_total": 0,
            "self_cpu_time_total": 0.9319999999988795,
            "input_shapes": "[[1, 2], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 2,
            "cpu_time_total": 0.930000000000291,
            "cuda_time_total": 0,
            "self_cpu_time_total": 0.930000000000291,
            "input_shapes": "[[1, 1, 2], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 1.0820000000003347,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.0820000000003347,
            "input_shapes": "[[1, 64, 2], [], [], []]",
            "stack": [
                "<built-in method transpose of Tensor object at 0x7efa43e4bb60>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 58.624000000001615,
            "cuda_time_total": 0,
            "self_cpu_time_total": 58.624000000001615,
            "input_shapes": "[[4096, 2560], [], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_0",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 86.23099999998158,
            "cuda_time_total": 0,
            "self_cpu_time_total": 86.23099999998158,
            "input_shapes": "[[1, 2, 32, 128], [], [], []]",
            "stack": [
                "<built-in method transpose of Tensor object at 0x7efa43e4bb60>",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 54.996000000015556,
            "cuda_time_total": 0,
            "self_cpu_time_total": 54.996000000015556,
            "input_shapes": "[[1024, 2560], [], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 100.86899999992784,
            "cuda_time_total": 0,
            "self_cpu_time_total": 100.86899999992784,
            "input_shapes": "[[1, 2, 8, 128], [], [], []]",
            "stack": [
                "<built-in method transpose of Tensor object at 0x7efa43e4bb60>",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 42.2720000000063,
            "cuda_time_total": 0,
            "self_cpu_time_total": 42.2720000000063,
            "input_shapes": "[[1, 2, 128], [], [], []]",
            "stack": [
                "<built-in method unsqueeze of Tensor object at 0x7efa43e4da40>",
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 144,
            "cpu_time_total": 130.30799999998635,
            "cuda_time_total": 0,
            "self_cpu_time_total": 130.30799999998635,
            "input_shapes": "[[1, 32, 2, 128], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(86): rotate_half",
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 144,
            "cpu_time_total": 115.62300000004143,
            "cuda_time_total": 0,
            "self_cpu_time_total": 115.62300000004143,
            "input_shapes": "[[1, 8, 2, 128], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(86): rotate_half",
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 29.836999999997715,
            "cuda_time_total": 0,
            "self_cpu_time_total": 29.836999999997715,
            "input_shapes": "[[2560, 4096], [], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_3",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 98.78000000005704,
            "cuda_time_total": 0,
            "self_cpu_time_total": 98.78000000005704,
            "input_shapes": "[[9728, 2560], [], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_4",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 45.753000000011525,
            "cuda_time_total": 0,
            "self_cpu_time_total": 45.753000000011525,
            "input_shapes": "[[2560, 9728], [], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_6",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 3,
            "cpu_time_total": 2.4550000000017462,
            "cuda_time_total": 0,
            "self_cpu_time_total": 2.4550000000017462,
            "input_shapes": "[[1, 2, 2560], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 0.9009999999980209,
            "cuda_time_total": 0,
            "self_cpu_time_total": 0.9009999999980209,
            "input_shapes": "[[151936, 2560], [], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_252",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::slice": [
        {
            "count": 1,
            "cpu_time_total": 5.560999999999694,
            "cuda_time_total": 0,
            "self_cpu_time_total": 4.9299999999993815,
            "input_shapes": "[[1, 64], [], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 4.2280000000000655,
            "cuda_time_total": 0,
            "self_cpu_time_total": 3.6870000000008076,
            "input_shapes": "[[1, 2], [], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 3.076000000000022,
            "cuda_time_total": 0,
            "self_cpu_time_total": 2.655999999999949,
            "input_shapes": "[[1, 1, 2], [], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 358.3920000000162,
            "cuda_time_total": 0,
            "self_cpu_time_total": 297.325000000008,
            "input_shapes": "[[1, 32, 2, 128], [], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(86): rotate_half",
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 365.0990000000711,
            "cuda_time_total": 0,
            "self_cpu_time_total": 299.1690000000508,
            "input_shapes": "[[1, 8, 2, 128], [], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(86): rotate_half",
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 3,
            "cpu_time_total": 15.229000000006636,
            "cuda_time_total": 0,
            "self_cpu_time_total": 12.77400000000489,
            "input_shapes": "[[1, 2, 2560], [], [], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::to": [
        {
            "count": 2,
            "cpu_time_total": 0.6210000000000946,
            "cuda_time_total": 0,
            "self_cpu_time_total": 0.6210000000000946,
            "input_shapes": "[[1, 64, 1], [], [], [], []]",
            "stack": [
                "<built-in method float of Tensor object at 0x7efa43e4dc20>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 1.11200000000008,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.11200000000008,
            "input_shapes": "[[1, 64, 1], [], [], [], [], [], [], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 2,
            "cpu_time_total": 38.61400000000049,
            "cuda_time_total": 3.199999999999818,
            "self_cpu_time_total": 2.2250000000012733,
            "input_shapes": "[[1, 1, 2], [], [], [], []]",
            "stack": [
                "<built-in method float of Tensor object at 0x7efa43e4dc20>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 2,
            "cpu_time_total": 48.26100000000042,
            "cuda_time_total": 4.288000000000466,
            "self_cpu_time_total": 3.175999999999476,
            "input_shapes": "[[1, 2, 128], [], [], [], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 219,
            "cpu_time_total": 3964.8519999999135,
            "cuda_time_total": 601.1449999999313,
            "self_cpu_time_total": 277.43999999992593,
            "input_shapes": "[[1, 2, 2560], [], [], [], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 108,
            "cpu_time_total": 1975.3180000000502,
            "cuda_time_total": 279.03700000002755,
            "self_cpu_time_total": 134.75400000003174,
            "input_shapes": "[[1, 2, 32, 128], [], [], [], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 108,
            "cpu_time_total": 2006.4419999999918,
            "cuda_time_total": 279.70599999996375,
            "self_cpu_time_total": 130.1420000000162,
            "input_shapes": "[[1, 2, 8, 128], [], [], [], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_2",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 980.4830000000111,
            "cuda_time_total": 0,
            "self_cpu_time_total": 113.986000000059,
            "input_shapes": "[[0], [], [], [], [], []]",
            "stack": [
                "<built-in method tensor of type object at 0x7efccdac94a0>",
                "transformers/cache_utils.py(92): lazy_initialization",
                "transformers/cache_utils.py(98): update",
                "transformers/cache_utils.py(742): update",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::expand": [
        {
            "count": 2,
            "cpu_time_total": 11.631999999999607,
            "cuda_time_total": 0,
            "self_cpu_time_total": 9.948999999999614,
            "input_shapes": "[[1, 64, 1], [], []]",
            "stack": [
                "<built-in method expand of Tensor object at 0x7efa43e4dc20>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 3.0359999999991487,
            "cuda_time_total": 0,
            "self_cpu_time_total": 2.5259999999989304,
            "input_shapes": "[[1, 1, 2], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::_to_copy": [
        {
            "count": 1,
            "cpu_time_total": 36.388999999999214,
            "cuda_time_total": 3.199999999999818,
            "self_cpu_time_total": 8.635999999997694,
            "input_shapes": "[[1, 1, 2], [], [], [], [], [], []]",
            "stack": [
                "<built-in method float of Tensor object at 0x7efa43e4dc20>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 2,
            "cpu_time_total": 45.085000000000946,
            "cuda_time_total": 4.288000000000466,
            "self_cpu_time_total": 10.06800000000112,
            "input_shapes": "[[1, 2, 128], [], [], [], [], [], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 146,
            "cpu_time_total": 3687.4119999999875,
            "cuda_time_total": 601.1449999999313,
            "self_cpu_time_total": 899.8970000000027,
            "input_shapes": "[[1, 2, 2560], [], [], [], [], [], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 1840.5640000000185,
            "cuda_time_total": 279.03700000002755,
            "self_cpu_time_total": 382.82300000000396,
            "input_shapes": "[[1, 2, 32, 128], [], [], [], [], [], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 1876.2999999999756,
            "cuda_time_total": 279.70599999996375,
            "self_cpu_time_total": 426.4989999998761,
            "input_shapes": "[[1, 2, 8, 128], [], [], [], [], [], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_2",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 866.4969999999521,
            "cuda_time_total": 0,
            "self_cpu_time_total": 435.3299999999981,
            "input_shapes": "[[0], [], [], [], [], [], []]",
            "stack": [
                "<built-in method tensor of type object at 0x7efccdac94a0>",
                "transformers/cache_utils.py(92): lazy_initialization",
                "transformers/cache_utils.py(98): update",
                "transformers/cache_utils.py(742): update",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::empty_strided": [
        {
            "count": 401,
            "cpu_time_total": 2148.445000000006,
            "cuda_time_total": 0,
            "self_cpu_time_total": 2148.445000000006,
            "input_shapes": "[[], [], [], [], [], []]",
            "stack": [
                "<built-in method float of Tensor object at 0x7efa43e4dc20>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::copy_": [
        {
            "count": 1,
            "cpu_time_total": 21.200000000000728,
            "cuda_time_total": 3.199999999999818,
            "self_cpu_time_total": 8.846000000000458,
            "input_shapes": "[[1, 1, 2], [1, 1, 2], []]",
            "stack": [
                "<built-in method float of Tensor object at 0x7efa43e4dc20>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 2,
            "cpu_time_total": 24.54700000000048,
            "cuda_time_total": 4.288000000000466,
            "self_cpu_time_total": 11.261000000000422,
            "input_shapes": "[[1, 2, 128], [1, 2, 128], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 146,
            "cpu_time_total": 1982.910000000009,
            "cuda_time_total": 601.1449999999313,
            "self_cpu_time_total": 919.9549999999908,
            "input_shapes": "[[1, 2, 2560], [1, 2, 2560], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 1040.7989999999863,
            "cuda_time_total": 279.03700000002755,
            "self_cpu_time_total": 481.03299999997034,
            "input_shapes": "[[1, 2, 32, 128], [1, 2, 32, 128], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 1029.1920000000555,
            "cuda_time_total": 279.70599999996375,
            "self_cpu_time_total": 480.14400000001115,
            "input_shapes": "[[1, 2, 8, 128], [1, 2, 8, 128], []]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_2",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 135.52199999999903,
            "cuda_time_total": 0,
            "self_cpu_time_total": 135.52199999999903,
            "input_shapes": "[[0], [0], []]",
            "stack": [
                "<built-in method tensor of type object at 0x7efccdac94a0>",
                "transformers/cache_utils.py(92): lazy_initialization",
                "transformers/cache_utils.py(98): update",
                "transformers/cache_utils.py(742): update",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>, 4, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1> >(int, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1>)": [
        {
            "count": 146,
            "cpu_time_total": 0,
            "cuda_time_total": 784.7940000000208,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in method float of Tensor object at 0x7efa43e4dc20>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::matmul": [
        {
            "count": 1,
            "cpu_time_total": 107.27400000000034,
            "cuda_time_total": 3.3919999999998254,
            "self_cpu_time_total": 28.673000000001593,
            "input_shapes": "[[1, 64, 1], [1, 1, 2]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 1739.1280000000188,
            "cuda_time_total": 795.520000000035,
            "self_cpu_time_total": 342.3150000000078,
            "input_shapes": "[[1, 2, 2560], [2560, 4096]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_0",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 4205.916000000021,
            "cuda_time_total": 927.5459999999566,
            "self_cpu_time_total": 731.4949999999026,
            "input_shapes": "[[1, 2, 2560], [2560, 1024]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 2142.45100000003,
            "cuda_time_total": 880.9549999999781,
            "self_cpu_time_total": 375.5060000000449,
            "input_shapes": "[[1, 2, 4096], [4096, 2560]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_3",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 3467.767000000018,
            "cuda_time_total": 3209.3050000000476,
            "self_cpu_time_total": 817.7910000000375,
            "input_shapes": "[[1, 2, 2560], [2560, 9728]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_4",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 1758.1080000000147,
            "cuda_time_total": 1705.5770000000593,
            "self_cpu_time_total": 362.4699999999375,
            "input_shapes": "[[1, 2, 9728], [9728, 2560]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_6",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 51.538000000000466,
            "cuda_time_total": 546.3350000000064,
            "self_cpu_time_total": 11.512999999991735,
            "input_shapes": "[[1, 2, 2560], [2560, 151936]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_252",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::bmm": [
        {
            "count": 1,
            "cpu_time_total": 61.827000000000226,
            "cuda_time_total": 3.3919999999998254,
            "self_cpu_time_total": 36.42900000000009,
            "input_shapes": "[[1, 64, 1], [1, 1, 2]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void gemmk1_kernel<int, float, 256, 5, false, false, false, false, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float, 0>(cublasGemmk1Params<float, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float, biasType<cublasGemvTensorStridedBatched<float>::value_type, float>::type>)": [
        {
            "count": 1,
            "cpu_time_total": 0,
            "cuda_time_total": 3.3919999999998254,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::_unsafe_view": [
        {
            "count": 1,
            "cpu_time_total": 1.5830000000005384,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.5830000000005384,
            "input_shapes": "[[1, 64, 2], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 53.73300000004019,
            "cuda_time_total": 0,
            "self_cpu_time_total": 53.73300000004019,
            "input_shapes": "[[2, 4096], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_0",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 114.92600000003222,
            "cuda_time_total": 0,
            "self_cpu_time_total": 114.92600000003222,
            "input_shapes": "[[2, 1024], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 109.69499999998516,
            "cuda_time_total": 0,
            "self_cpu_time_total": 109.69499999998516,
            "input_shapes": "[[2, 2560], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_3",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 102.80399999998554,
            "cuda_time_total": 0,
            "self_cpu_time_total": 102.80399999998554,
            "input_shapes": "[[2, 9728], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_4",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 1.4729999999981374,
            "cuda_time_total": 0,
            "self_cpu_time_total": 1.4729999999981374,
            "input_shapes": "[[2, 151936], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_252",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::transpose": [
        {
            "count": 1,
            "cpu_time_total": 5.329999999999927,
            "cuda_time_total": 0,
            "self_cpu_time_total": 4.2479999999995925,
            "input_shapes": "[[1, 64, 2], [], []]",
            "stack": [
                "<built-in method transpose of Tensor object at 0x7efa43e4bb60>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 176.04899999999725,
            "cuda_time_total": 0,
            "self_cpu_time_total": 117.42499999999563,
            "input_shapes": "[[4096, 2560], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_0",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 341.5659999999534,
            "cuda_time_total": 0,
            "self_cpu_time_total": 255.33499999997184,
            "input_shapes": "[[1, 2, 32, 128], [], []]",
            "stack": [
                "<built-in method transpose of Tensor object at 0x7efa43e4bb60>",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 209.65000000003238,
            "cuda_time_total": 0,
            "self_cpu_time_total": 154.65400000001682,
            "input_shapes": "[[1024, 2560], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 442.03899999998976,
            "cuda_time_total": 0,
            "self_cpu_time_total": 341.1700000000619,
            "input_shapes": "[[1, 2, 8, 128], [], []]",
            "stack": [
                "<built-in method transpose of Tensor object at 0x7efa43e4bb60>",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 322.6399999999103,
            "cuda_time_total": 0,
            "self_cpu_time_total": 253.39899999993213,
            "input_shapes": "[[1, 32, 2, 128], [], []]",
            "stack": [
                "<built-in function scaled_dot_product_attention>",
                "transformers/integrations/sdpa_attention.py(48): sdpa_attention_forward",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 182.99899999997433,
            "cuda_time_total": 0,
            "self_cpu_time_total": 133.3059999999532,
            "input_shapes": "[[1, 8, 2, 128], [], []]",
            "stack": [
                "<built-in function scaled_dot_product_attention>",
                "transformers/integrations/sdpa_attention.py(48): sdpa_attention_forward",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 110.66000000001623,
            "cuda_time_total": 0,
            "self_cpu_time_total": 80.82300000001851,
            "input_shapes": "[[2560, 4096], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_3",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 321.078999999987,
            "cuda_time_total": 0,
            "self_cpu_time_total": 222.29899999992995,
            "input_shapes": "[[9728, 2560], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_4",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 147.54099999994833,
            "cuda_time_total": 0,
            "self_cpu_time_total": 101.7879999999368,
            "input_shapes": "[[2560, 9728], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_6",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 3.5570000000006985,
            "cuda_time_total": 0,
            "self_cpu_time_total": 2.6560000000026776,
            "input_shapes": "[[151936, 2560], [], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_252",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::cat": [
        {
            "count": 145,
            "cpu_time_total": 2829.2709999999606,
            "cuda_time_total": 907.3539999999721,
            "self_cpu_time_total": 1796.6429999999373,
            "input_shapes": "[[], []]",
            "stack": [
                "<built-in method cat of type object at 0x7efccdac94a0>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 3, 64, 64>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)": [
        {
            "count": 1,
            "cpu_time_total": 0,
            "cuda_time_total": 4.256000000001222,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in method cat of type object at 0x7efccdac94a0>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::cos": [
        {
            "count": 1,
            "cpu_time_total": 20.46899999999914,
            "cuda_time_total": 3.6159999999999854,
            "self_cpu_time_total": 12.494000000000597,
            "input_shapes": "[[1, 2, 128]]",
            "stack": [
                "<built-in method cos of Tensor object at 0x7efa43e4bac0>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::vectorized_elementwise_kernel<4, at::native::cos_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul> >(int, at::native::cos_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>)": [
        {
            "count": 1,
            "cpu_time_total": 0,
            "cuda_time_total": 3.6159999999999854,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in method cos of Tensor object at 0x7efa43e4bac0>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::mul": [
        {
            "count": 2,
            "cpu_time_total": 38.70399999999972,
            "cuda_time_total": 4.735999999998967,
            "self_cpu_time_total": 22.1820000000007,
            "input_shapes": "[[1, 2, 128], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 73,
            "cpu_time_total": 1139.135999999984,
            "cuda_time_total": 284.5120000000097,
            "self_cpu_time_total": 694.3949999999659,
            "input_shapes": "[[1, 2, 2560], [1, 2, 1]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 73,
            "cpu_time_total": 1142.3199999999943,
            "cuda_time_total": 416.05999999996857,
            "self_cpu_time_total": 703.0669999999245,
            "input_shapes": "[[2560], [1, 2, 2560]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 570.8849999999984,
            "cuda_time_total": 142.72299999999268,
            "self_cpu_time_total": 351.78099999995175,
            "input_shapes": "[[1, 2, 32, 128], [1, 2, 32, 1]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 759.1940000000177,
            "cuda_time_total": 184.8659999999818,
            "self_cpu_time_total": 544.4839999999913,
            "input_shapes": "[[128], [1, 2, 32, 128]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 559.9939999999951,
            "cuda_time_total": 140.06599999999344,
            "self_cpu_time_total": 342.1269999999913,
            "input_shapes": "[[1, 2, 8, 128], [1, 2, 8, 1]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_2",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 557.9879999999721,
            "cuda_time_total": 176.1300000000283,
            "self_cpu_time_total": 342.0390000000025,
            "input_shapes": "[[128], [1, 2, 8, 128]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_2",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 1411.6199999999662,
            "cuda_time_total": 343.0350000000217,
            "self_cpu_time_total": 872.2149999999565,
            "input_shapes": "[[1, 32, 2, 128], [1, 1, 2, 128]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 1083.737000000012,
            "cuda_time_total": 325.1239999999925,
            "self_cpu_time_total": 648.4010000000198,
            "input_shapes": "[[1, 8, 2, 128], [1, 1, 2, 128]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 645.2319999999872,
            "cuda_time_total": 112.7979999999734,
            "self_cpu_time_total": 390.49999999998363,
            "input_shapes": "[[1, 2, 9728], [1, 2, 9728]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)": [
        {
            "count": 2,
            "cpu_time_total": 0,
            "cuda_time_total": 4.735999999998967,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::sin": [
        {
            "count": 1,
            "cpu_time_total": 19.266999999999825,
            "cuda_time_total": 3.584000000000742,
            "self_cpu_time_total": 11.591999999998734,
            "input_shapes": "[[1, 2, 128]]",
            "stack": [
                "<built-in method sin of Tensor object at 0x7efa43e4bac0>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::vectorized_elementwise_kernel<4, at::native::sin_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul> >(int, at::native::sin_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>)": [
        {
            "count": 1,
            "cpu_time_total": 0,
            "cuda_time_total": 3.584000000000742,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in method sin of Tensor object at 0x7efa43e4bac0>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::vectorized_elementwise_kernel<4, at::native::float16_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda(float)#1}, std::array<char*, 2ul> >(int, at::native::float16_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda(float)#1}, std::array<char*, 2ul>)": [
        {
            "count": 147,
            "cpu_time_total": 0,
            "cuda_time_total": 382.5819999999021,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in method to of Tensor object at 0x7efa43e4e8f0>",
                "transformers/models/qwen3/modeling_qwen3.py(319): forward",
                "transformers/modeling_rope_utils.py(81): wrapper",
                "torch/utils/_contextlib.py(117): decorate_context",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RotaryEmbedding_0",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::pow": [
        {
            "count": 73,
            "cpu_time_total": 1410.099000000002,
            "cuda_time_total": 193.57099999997445,
            "self_cpu_time_total": 910.662000000124,
            "input_shapes": "[[1, 2, 2560], []]",
            "stack": [
                "<built-in method pow of Tensor object at 0x7efa43e4b3e0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 689.9369999999963,
            "cuda_time_total": 96.19599999994716,
            "self_cpu_time_total": 442.6429999999382,
            "input_shapes": "[[1, 2, 32, 128], []]",
            "stack": [
                "<built-in method pow of Tensor object at 0x7efa43e4b3e0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 713.5100000000002,
            "cuda_time_total": 91.99899999995796,
            "self_cpu_time_total": 468.35799999998017,
            "input_shapes": "[[1, 2, 8, 128], []]",
            "stack": [
                "<built-in method pow of Tensor object at 0x7efa43e4b3e0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_2",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::result_type": [
        {
            "count": 73,
            "cpu_time_total": 31.487999999962994,
            "cuda_time_total": 0,
            "self_cpu_time_total": 31.487999999962994,
            "input_shapes": "[[1, 2, 2560], []]",
            "stack": [
                "<built-in method pow of Tensor object at 0x7efa43e4b3e0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 15.213000000019747,
            "cuda_time_total": 0,
            "self_cpu_time_total": 15.213000000019747,
            "input_shapes": "[[1, 2, 32, 128], []]",
            "stack": [
                "<built-in method pow of Tensor object at 0x7efa43e4b3e0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 14.55100000001039,
            "cuda_time_total": 0,
            "self_cpu_time_total": 14.55100000001039,
            "input_shapes": "[[1, 2, 8, 128], []]",
            "stack": [
                "<built-in method pow of Tensor object at 0x7efa43e4b3e0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_2",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#1}, std::array<char*, 2ul> >(int, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#1}, std::array<char*, 2ul>)": [
        {
            "count": 145,
            "cpu_time_total": 0,
            "cuda_time_total": 381.76599999987957,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in method pow of Tensor object at 0x7efa43e4b3e0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::mean": [
        {
            "count": 73,
            "cpu_time_total": 1373.114999999978,
            "cuda_time_total": 430.0790000000743,
            "self_cpu_time_total": 917.4479999999585,
            "input_shapes": "[[1, 2, 2560], [], [], []]",
            "stack": [
                "<built-in method mean of Tensor object at 0x7efa43e4b340>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 664.6780000000435,
            "cuda_time_total": 202.55900000000474,
            "self_cpu_time_total": 439.7390000000323,
            "input_shapes": "[[1, 2, 32, 128], [], [], []]",
            "stack": [
                "<built-in method mean of Tensor object at 0x7efa43e4b340>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 665.6140000000305,
            "cuda_time_total": 173.72499999996762,
            "self_cpu_time_total": 442.221000000025,
            "input_shapes": "[[1, 2, 8, 128], [], [], []]",
            "stack": [
                "<built-in method mean of Tensor object at 0x7efa43e4b340>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_2",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4, 4> >(at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4, 4>)": [
        {
            "count": 145,
            "cpu_time_total": 0,
            "cuda_time_total": 806.3630000000467,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in method mean of Tensor object at 0x7efa43e4b340>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::add": [
        {
            "count": 73,
            "cpu_time_total": 1205.8429999999717,
            "cuda_time_total": 169.21300000002702,
            "self_cpu_time_total": 732.4339999999956,
            "input_shapes": "[[1, 2, 1], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 602.9450000000161,
            "cuda_time_total": 79.57999999999447,
            "self_cpu_time_total": 370.7569999999778,
            "input_shapes": "[[1, 2, 32, 1], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 606.3909999999578,
            "cuda_time_total": 74.65600000002632,
            "self_cpu_time_total": 373.7189999999446,
            "input_shapes": "[[1, 2, 8, 1], [], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_2",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 560.3120000000054,
            "cuda_time_total": 185.89100000005965,
            "self_cpu_time_total": 347.2699999999804,
            "input_shapes": "[[1, 32, 2, 128], [1, 32, 2, 128], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 533.1210000000428,
            "cuda_time_total": 164.06500000001506,
            "self_cpu_time_total": 326.3890000000338,
            "input_shapes": "[[1, 8, 2, 128], [1, 8, 2, 128], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 1316.96900000001,
            "cuda_time_total": 214.40100000000893,
            "self_cpu_time_total": 809.0440000000381,
            "input_shapes": "[[1, 2, 2560], [1, 2, 2560], []]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<float>, std::array<char*, 2ul> >(int, at::native::CUDAFunctorOnSelf_add<float>, std::array<char*, 2ul>)": [
        {
            "count": 145,
            "cpu_time_total": 0,
            "cuda_time_total": 323.4490000000478,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::rsqrt": [
        {
            "count": 73,
            "cpu_time_total": 995.3650000000089,
            "cuda_time_total": 180.90199999998913,
            "self_cpu_time_total": 577.5609999999979,
            "input_shapes": "[[1, 2, 1]]",
            "stack": [
                "<built-in method rsqrt of type object at 0x7efccdac94a0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 492.91499999999905,
            "cuda_time_total": 82.87899999998263,
            "self_cpu_time_total": 291.15100000002894,
            "input_shapes": "[[1, 2, 32, 1]]",
            "stack": [
                "<built-in method rsqrt of type object at 0x7efccdac94a0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 492.35300000001916,
            "cuda_time_total": 82.97300000001633,
            "self_cpu_time_total": 287.2820000000065,
            "input_shapes": "[[1, 2, 8, 1]]",
            "stack": [
                "<built-in method rsqrt of type object at 0x7efccdac94a0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_2",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::vectorized_elementwise_kernel<4, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul> >(int, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>)": [
        {
            "count": 145,
            "cpu_time_total": 0,
            "cuda_time_total": 346.7539999999881,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in method rsqrt of type object at 0x7efccdac94a0>",
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})": [
        {
            "count": 145,
            "cpu_time_total": 0,
            "cuda_time_total": 567.3009999999958,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})": [
        {
            "count": 289,
            "cpu_time_total": 0,
            "cuda_time_total": 1445.2149999999929,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(59): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3RMSNorm_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::linear": [
        {
            "count": 36,
            "cpu_time_total": 2142.729000000021,
            "cuda_time_total": 795.520000000035,
            "self_cpu_time_total": 114.77699999998913,
            "input_shapes": "[[1, 2, 2560], [4096, 2560], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_0",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 4873.161000000009,
            "cuda_time_total": 927.5459999999566,
            "self_cpu_time_total": 281.196000000009,
            "input_shapes": "[[1, 2, 2560], [1024, 2560], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 2460.628000000006,
            "cuda_time_total": 880.9549999999781,
            "self_cpu_time_total": 113.95799999996234,
            "input_shapes": "[[1, 2, 4096], [2560, 4096], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_3",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 4218.146000000012,
            "cuda_time_total": 3209.3050000000476,
            "self_cpu_time_total": 217.68400000006113,
            "input_shapes": "[[1, 2, 2560], [9728, 2560], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_4",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 2105.1430000000437,
            "cuda_time_total": 1705.5770000000593,
            "self_cpu_time_total": 105.45000000001164,
            "input_shapes": "[[1, 2, 9728], [2560, 9728], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_6",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 60.75500000000466,
            "cuda_time_total": 546.3350000000064,
            "self_cpu_time_total": 3.1150000000052387,
            "input_shapes": "[[1, 2, 2560], [151936, 2560], []]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_252",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::t": [
        {
            "count": 36,
            "cpu_time_total": 288.82400000001326,
            "cuda_time_total": 0,
            "self_cpu_time_total": 112.77500000001601,
            "input_shapes": "[[4096, 2560]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_0",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 386.04899999997906,
            "cuda_time_total": 0,
            "self_cpu_time_total": 176.39899999994668,
            "input_shapes": "[[1024, 2560]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 204.2190000000137,
            "cuda_time_total": 0,
            "self_cpu_time_total": 93.55899999999747,
            "input_shapes": "[[2560, 4096]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_3",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 532.6949999999324,
            "cuda_time_total": 0,
            "self_cpu_time_total": 211.61599999994542,
            "input_shapes": "[[9728, 2560]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_4",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 241.58500000001732,
            "cuda_time_total": 0,
            "self_cpu_time_total": 94.04400000006899,
            "input_shapes": "[[2560, 9728]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_6",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 6.101999999998952,
            "cuda_time_total": 0,
            "self_cpu_time_total": 2.5449999999982538,
            "input_shapes": "[[151936, 2560]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_252",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::mm": [
        {
            "count": 36,
            "cpu_time_total": 1194.9789999999757,
            "cuda_time_total": 795.520000000035,
            "self_cpu_time_total": 827.216999999946,
            "input_shapes": "[[2, 2560], [2560, 4096]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_0",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 3050.6830000000245,
            "cuda_time_total": 927.5459999999566,
            "self_cpu_time_total": 2024.3099999999595,
            "input_shapes": "[[2, 2560], [2560, 1024]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 1606.9299999999603,
            "cuda_time_total": 880.9549999999781,
            "self_cpu_time_total": 1062.8479999999327,
            "input_shapes": "[[2, 4096], [4096, 2560]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_3",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 72,
            "cpu_time_total": 2222.660000000038,
            "cuda_time_total": 3209.3050000000476,
            "self_cpu_time_total": 1500.5850000000428,
            "input_shapes": "[[2, 2560], [2560, 9728]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_4",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 1209.5640000000185,
            "cuda_time_total": 1705.5770000000593,
            "self_cpu_time_total": 736.3070000000334,
            "input_shapes": "[[2, 9728], [9728, 2560]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_6",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 1,
            "cpu_time_total": 34.47500000000582,
            "cuda_time_total": 546.3350000000064,
            "self_cpu_time_total": 23.545000000012806,
            "input_shapes": "[[2, 2560], [2560, 151936]]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_252",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "cudaOccupancyMaxActiveBlocksPerMultiprocessor": [
        {
            "count": 108,
            "cpu_time_total": 148.61300000001575,
            "cuda_time_total": 0,
            "self_cpu_time_total": 148.61300000001575,
            "input_shapes": "[]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_0",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f_stages_64x6_tn": [
        {
            "count": 36,
            "cpu_time_total": 0,
            "cuda_time_total": 795.520000000035,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_0",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "cudaDeviceGetAttribute": [
        {
            "count": 180,
            "cpu_time_total": 82.2430000001259,
            "cuda_time_total": 0,
            "self_cpu_time_total": 82.2430000001259,
            "input_shapes": "[]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "cuLaunchKernel": [
        {
            "count": 108,
            "cpu_time_total": 926.7910000000538,
            "cuda_time_total": 0,
            "self_cpu_time_total": 926.7910000000538,
            "input_shapes": "[]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm_f16_64x64_64x6_tn_align8>(cutlass_80_tensorop_s16816gemm_f16_64x64_64x6_tn_align8::Params)": [
        {
            "count": 108,
            "cpu_time_total": 0,
            "cuda_time_total": 1447.7039999999633,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void cublasLt::splitKreduce_kernel<32, 16, int, float, __half, float, __half, false, __half, __half, __half, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, __half const*, __half*, __half*, float const*, float const*, __half const*, float const*, __half*, void*, long, float*, int*, float*, float*, float const*, float const*, float const*, float const*, float const*)": [
        {
            "count": 108,
            "cpu_time_total": 0,
            "cuda_time_total": 360.7969999999714,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_1",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::neg": [
        {
            "count": 36,
            "cpu_time_total": 648.5659999999698,
            "cuda_time_total": 174.5899999999965,
            "self_cpu_time_total": 378.29299999997056,
            "input_shapes": "[[1, 32, 2, 64]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(86): rotate_half",
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        },
        {
            "count": 36,
            "cpu_time_total": 603.489000000016,
            "cuda_time_total": 158.4349999999613,
            "self_cpu_time_total": 340.5699999999979,
            "input_shapes": "[[1, 8, 2, 64]]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(86): rotate_half",
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1}>(at::TensorIteratorBase&, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(c10::Half)#1} const&)::{lambda(int)#1})": [
        {
            "count": 72,
            "cpu_time_total": 0,
            "cuda_time_total": 333.0249999999578,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(86): rotate_half",
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::(anonymous namespace)::CatArrayBatchedCopy<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 4, 64, 64>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 64, 64>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)": [
        {
            "count": 144,
            "cpu_time_total": 0,
            "cuda_time_total": 903.0979999999709,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in method cat of type object at 0x7efccdac94a0>",
                "transformers/models/qwen3/modeling_qwen3.py(86): rotate_half",
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::Half> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::Half> const&)::{lambda(int)#1})": [
        {
            "count": 72,
            "cpu_time_total": 0,
            "cuda_time_total": 349.9560000000747,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(93): apply_rotary_pos_emb",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::lift_fresh": [
        {
            "count": 72,
            "cpu_time_total": 12.014999999992142,
            "cuda_time_total": 0,
            "self_cpu_time_total": 12.014999999992142,
            "input_shapes": "[[0]]",
            "stack": [
                "<built-in method tensor of type object at 0x7efccdac94a0>",
                "transformers/cache_utils.py(92): lazy_initialization",
                "transformers/cache_utils.py(98): update",
                "transformers/cache_utils.py(742): update",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::detach_": [
        {
            "count": 72,
            "cpu_time_total": 134.300000000012,
            "cuda_time_total": 0,
            "self_cpu_time_total": 100.13600000004226,
            "input_shapes": "[[0]]",
            "stack": [
                "<built-in method tensor of type object at 0x7efccdac94a0>",
                "transformers/cache_utils.py(92): lazy_initialization",
                "transformers/cache_utils.py(98): update",
                "transformers/cache_utils.py(742): update",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "detach_": [
        {
            "count": 72,
            "cpu_time_total": 34.16399999996975,
            "cuda_time_total": 0,
            "self_cpu_time_total": 34.16399999996975,
            "input_shapes": "[[0]]",
            "stack": [
                "<built-in method tensor of type object at 0x7efccdac94a0>",
                "transformers/cache_utils.py(92): lazy_initialization",
                "transformers/cache_utils.py(98): update",
                "transformers/cache_utils.py(742): update",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::scaled_dot_product_attention": [
        {
            "count": 36,
            "cpu_time_total": 3068.4390000000203,
            "cuda_time_total": 551.0339999999742,
            "self_cpu_time_total": 393.49300000004223,
            "input_shapes": "[[1, 32, 2, 128], [1, 8, 2, 128], [1, 8, 2, 128], [], [], [], [], []]",
            "stack": [
                "<built-in function scaled_dot_product_attention>",
                "transformers/integrations/sdpa_attention.py(48): sdpa_attention_forward",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::_scaled_dot_product_flash_attention": [
        {
            "count": 36,
            "cpu_time_total": 2674.945999999978,
            "cuda_time_total": 551.0339999999742,
            "self_cpu_time_total": 381.83600000002116,
            "input_shapes": "[[1, 32, 2, 128], [1, 8, 2, 128], [1, 8, 2, 128], [], [], [], []]",
            "stack": [
                "<built-in function scaled_dot_product_attention>",
                "transformers/integrations/sdpa_attention.py(48): sdpa_attention_forward",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::_flash_attention_forward": [
        {
            "count": 36,
            "cpu_time_total": 1827.6520000000073,
            "cuda_time_total": 551.0339999999742,
            "self_cpu_time_total": 600.7109999998629,
            "input_shapes": "[[1, 2, 32, 128], [1, 2, 8, 128], [1, 2, 8, 128], [], [], [], [], [], [], [], [], [], [], [], []]",
            "stack": [
                "<built-in function scaled_dot_product_attention>",
                "transformers/integrations/sdpa_attention.py(48): sdpa_attention_forward",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::empty_like": [
        {
            "count": 36,
            "cpu_time_total": 288.90300000000207,
            "cuda_time_total": 0,
            "self_cpu_time_total": 95.28199999999924,
            "input_shapes": "[[1, 2, 32, 128], [], [], [], [], []]",
            "stack": [
                "<built-in function scaled_dot_product_attention>",
                "transformers/integrations/sdpa_attention.py(48): sdpa_attention_forward",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "cudaFuncSetAttribute": [
        {
            "count": 36,
            "cpu_time_total": 54.92800000005627,
            "cuda_time_total": 0,
            "self_cpu_time_total": 54.92800000005627,
            "input_shapes": "[]",
            "stack": [
                "<built-in function scaled_dot_product_attention>",
                "transformers/integrations/sdpa_attention.py(48): sdpa_attention_forward",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void pytorch_flash::flash_fwd_kernel<Flash_fwd_kernel_traits<128, 128, 64, 4, false, false, cutlass::half_t, Flash_kernel_traits<128, 128, 64, 4, cutlass::half_t> >, false, true, false, false, false, true, false, false>(pytorch_flash::Flash_fwd_params)": [
        {
            "count": 36,
            "cpu_time_total": 0,
            "cuda_time_total": 551.0339999999742,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in function scaled_dot_product_attention>",
                "transformers/integrations/sdpa_attention.py(48): sdpa_attention_forward",
                "transformers/models/qwen3/modeling_qwen3.py(187): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Attention_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::Half>, std::array<char*, 3ul>)": [
        {
            "count": 72,
            "cpu_time_total": 0,
            "cuda_time_total": 214.40100000000893,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f_stages_64x5_tn": [
        {
            "count": 73,
            "cpu_time_total": 0,
            "cuda_time_total": 3755.640000000054,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_4",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "aten::silu": [
        {
            "count": 36,
            "cpu_time_total": 653.5780000000141,
            "cuda_time_total": 131.42499999999382,
            "self_cpu_time_total": 379.9360000000397,
            "input_shapes": "[[1, 2, 9728]]",
            "stack": [
                "<built-in function silu>",
                "torch/nn/functional.py(2353): silu",
                "transformers/activations.py(98): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: SiLUActivation_0",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::silu_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#5}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul> >(int, at::native::(anonymous namespace)::silu_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#5}::operator()() const::{lambda(c10::Half)#1}, std::array<char*, 2ul>)": [
        {
            "count": 36,
            "cpu_time_total": 0,
            "cuda_time_total": 131.42499999999382,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in function silu>",
                "torch/nn/functional.py(2353): silu",
                "transformers/activations.py(98): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: SiLUActivation_0",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<c10::Half, c10::Half, c10::Half, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)": [
        {
            "count": 36,
            "cpu_time_total": 0,
            "cuda_time_total": 112.7979999999734,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64x6_tn": [
        {
            "count": 36,
            "cpu_time_total": 0,
            "cuda_time_total": 1565.7650000000303,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_6",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "void cublasLt::splitKreduce_kernel<32, 16, int, __half, __half, float, __half, false, __half, __half, __half, true, false, false>(cublasLt::cublasSplitKParams<float>, __half const*, __half const*, __half*, __half*, float const*, float const*, __half const*, __half const*, __half*, void*, long, float*, int*, float*, float*, float const*, float const*, float const*, float const*, float const*)": [
        {
            "count": 36,
            "cpu_time_total": 0,
            "cuda_time_total": 139.812000000029,
            "self_cpu_time_total": 0,
            "input_shapes": "[]",
            "stack": [
                "<built-in function linear>",
                "torch/nn/modules/linear.py(124): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Linear_6",
                "transformers/models/qwen3/modeling_qwen3.py(81): forward",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3MLP_0",
                "transformers/models/qwen3/modeling_qwen3.py(245): forward",
                "transformers/utils/deprecation.py(120): wrapped_func",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3DecoderLayer_0",
                "transformers/modeling_layers.py(60): __call__",
                "transformers/models/qwen3/modeling_qwen3.py(354): forward",
                "transformers/utils/generic.py(953): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3Model_0",
                "transformers/models/qwen3/modeling_qwen3.py(443): forward",
                "transformers/utils/generic.py(912): wrapper",
                "torch/nn/modules/module.py(1777): _call_impl",
                "nn.Module: Qwen3ForCausalLM_0",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(42): _forward",
                "/share/project/hdx/Git/kerneltracer/execution/huggingface_execution.py(22): execute",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ],
    "cudaDeviceSynchronize": [
        {
            "count": 1,
            "cpu_time_total": 92.49499999999534,
            "cuda_time_total": 0,
            "self_cpu_time_total": 92.49499999999534,
            "input_shapes": "[]",
            "stack": [
                "<built-in function _cuda_synchronize>",
                "torch/cuda/__init__.py(1075): synchronize",
                "torch/autograd/profiler.py(375): __exit__",
                "torch/profiler/profiler.py(251): stop_trace",
                "torch/profiler/profiler.py(874): _transit_action",
                "torch/profiler/profiler.py(834): step",
                "/share/project/hdx/Git/kerneltracer/tracers/torch_profiler.py(76): run",
                "<function trace_all at 0x7efa43fa07c0>",
                "/share/project/hdx/Git/kerneltracer/tracers/tracer_all.py(55): trace_all",
                "<function main at 0x7efa72501580>",
                "worker.py(124): main",
                "worker.py(163): <module>"
            ]
        }
    ]
}